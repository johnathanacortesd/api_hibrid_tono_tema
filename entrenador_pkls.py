# -*- coding: utf-8 -*-
"""Entrenador_pkls.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oe9ThUGIkGA5_QQzycErE2R530MchcFC

#üí™üîß Entrenador de modelos pkl versi√≥n 13/11/2025
"""

# @title Concatenador y unificador para xlsx de entrenamiento
# Instalar dependencias
!pip install -q polars openpyxl xlsxwriter ipywidgets

import polars as pl
import ipywidgets as widgets
from IPython.display import display, clear_output
from google.colab import files
import io

print("üöÄ Concatenador XLSX Ultra-F√°cil Cargado!\n")

# ============================================================================
# VARIABLES GLOBALES
# ============================================================================
uploaded_files = {}
current_file_index = 0
all_mappings = {}
output_area = widgets.Output()

# ============================================================================
# PASO 1: SUBIR ARCHIVOS
# ============================================================================
def step1_upload(b=None): # b=None permite llamarlo desde un bot√≥n
    """Limpia la pantalla y muestra el widget de subida de archivos."""
    with output_area:
        clear_output()
        print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
        print("‚ïë       üìä CONCATENADOR XLSX - S√öPER F√ÅCIL üìä              ‚ïë")
        print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n")
        print("üìÅ PASO 1: Selecciona todos tus archivos XLSX\n")
        print("Mant√©n presionada la tecla 'Ctrl' (o 'Cmd' en Mac) para seleccionar varios a la vez.")

        uploaded = files.upload()

        global uploaded_files
        uploaded_files = {}

        if not uploaded:
            print("\n‚ùå No se seleccion√≥ ning√∫n archivo. Haz clic de nuevo en el bot√≥n para intentarlo.")
            return

        print("\n‚è≥ Procesando archivos cargados...")
        for filename, content in uploaded.items():
            if filename.endswith(('.xlsx', '.xls')):
                try:
                    df = pl.read_excel(io.BytesIO(content))
                    uploaded_files[filename] = df
                    print(f"‚úÖ {filename} ({df.height:,} filas)")
                except Exception as e:
                    print(f"‚ùå Error al leer el archivo {filename}: {e}")

        if uploaded_files:
            print(f"\n‚ú® ¬°√âxito! {len(uploaded_files)} archivo(s) cargado(s) correctamente.")
            print("\n‚è© Ahora, vamos a mapear las columnas de cada archivo...\n")
            step2_map_columns()
        else:
            print("\n‚ùå No se cargaron archivos XLSX v√°lidos. Por favor, int√©ntalo de nuevo.")

# ============================================================================
# PASO 2: MAPEAR COLUMNAS (INTERACTIVO)
# ============================================================================
def step2_map_columns():
    """Inicializa el proceso de mapeo de columnas."""
    global current_file_index, all_mappings
    current_file_index = 0
    all_mappings = {}
    show_next_file()

def show_next_file(message=None):
    """Muestra la interfaz de mapeo para el archivo actual, con la opci√≥n de omitir."""
    global current_file_index

    with output_area:
        clear_output(wait=True)

        # Muestra el mensaje de la acci√≥n anterior (si existe)
        if message:
            print(message)

        file_list = list(uploaded_files.keys())

        if not file_list or current_file_index >= len(file_list):
            generate_final_file()
            return

        filename = file_list[current_file_index]
        df = uploaded_files[filename]

        # El t√≠tulo siempre mostrar√° el progreso correcto, incluso si se omiten archivos
        print("="*70)
        print(f"üìÑ MAPEO DE ARCHIVO {current_file_index + 1}/{len(file_list)}: {filename}")
        print("="*70)
        print(f"\nüìä {df.height:,} filas √ó {df.width} columnas\n")

        print("üëÄ VISTA PREVIA (primeras 5 filas):")
        print("-"*70)
        display(df.head(5).to_pandas())
        print("-"*70)
        print(f"\n‚úèÔ∏è Columnas disponibles en este archivo: {', '.join(df.columns)}\n")

        cols = ['(No usar)'] + df.columns
        titulo_dd = widgets.Dropdown(options=cols, description='üìå T√çTULO:', style={'description_width': '120px'}, layout=widgets.Layout(width='90%'))
        resumen_dd = widgets.Dropdown(options=cols, description='üìù RESUMEN:', style={'description_width': '120px'}, layout=widgets.Layout(width='90%'))
        tono_dd = widgets.Dropdown(options=cols, description='üòä TONO:', style={'description_width': '120px'}, layout=widgets.Layout(width='90%'))
        tema_dd = widgets.Dropdown(options=cols, description='üè∑Ô∏è TEMA:', style={'description_width': '120px'}, layout=widgets.Layout(width='90%'))

        auto_detect(df.columns, titulo_dd, resumen_dd, tono_dd, tema_dd)

        # Determinar el texto del bot√≥n "Siguiente"
        is_last_file = (current_file_index + 1) >= len(file_list)
        next_btn_text = 'üèÅ Finalizar Mapeo y Generar' if is_last_file else '‚úÖ Guardar y Siguiente'

        # Bot√≥n para GUARDAR y pasar al siguiente
        next_btn = widgets.Button(
            description=next_btn_text,
            button_style='success',
            icon='arrow-right',
            layout=widgets.Layout(width='auto', height='45px'),
        )

        # <<-- NUEVO: Bot√≥n para OMITIR el archivo actual -->>
        omit_btn = widgets.Button(
            description="Omitir este archivo",
            button_style='danger', # Rojo para indicar una acci√≥n de descarte
            icon='trash',
            layout=widgets.Layout(width='auto', height='45px'),
        )

        def on_next_click(b):
            all_mappings[filename] = {
                'titulo': titulo_dd.value,
                'resumen': resumen_dd.value,
                'tono': tono_dd.value,
                'tema': tema_dd.value
            }
            global current_file_index
            current_file_index += 1
            show_next_file(message=f"‚úÖ Mapeo de '{filename}' guardado. Mostrando el siguiente...")

        # <<-- NUEVO: L√≥gica para el bot√≥n de omitir -->>
        def on_omit_click(b):
            global uploaded_files, current_file_index
            # Elimina el archivo del diccionario para que no se procese
            del uploaded_files[filename]
            # No incrementamos el √≠ndice, porque la lista se acorta.
            # La siguiente llamada a show_next_file usar√° el mismo √≠ndice
            # para el "nuevo" archivo que ahora ocupa esa posici√≥n.
            show_next_file(message=f"üóëÔ∏è Archivo '{filename}' omitido. Mostrando el siguiente...")

        next_btn.on_click(on_next_click)
        omit_btn.on_click(on_omit_click) # Asignar la funci√≥n al nuevo bot√≥n

        print("\n" + "="*70)
        print("‚öôÔ∏è ASIGNA LAS COLUMNAS O DESCARTA EL ARCHIVO:")
        print("="*70)
        print("üí° Nota: El 'T√çTULO' y 'RESUMEN' se unir√°n en la columna final 'resumen'.\n")

        display(widgets.VBox([
            widgets.HTML("<div style='background: #e3f2fd; padding: 10px; border-radius: 5px; margin-bottom: 10px;'>"
                        "<b>Basado en la vista previa, selecciona las columnas o haz clic en 'Omitir'.</b></div>"),
            titulo_dd,
            resumen_dd,
            tono_dd,
            tema_dd,
            widgets.HTML("<br>"),
            # <<-- NUEVO: HBox para poner los botones uno al lado del otro -->>
            widgets.HBox([omit_btn, next_btn])
        ]))

def auto_detect(columns, titulo_w, resumen_w, tono_w, tema_w):
    """Intenta auto-seleccionar las columnas en los dropdowns bas√°ndose en palabras clave."""
    cols_lower = {c.lower(): c for c in columns}
    for kw in ['titulo', 't√≠tulo', 'title', 'nombre', 'headline', 'encabezado']:
        for cl, co in cols_lower.items():
            if kw in cl: titulo_w.value = co; break
    for kw in ['resumen', 'summary', 'descripcion', 'descripci√≥n', 'texto', 'content', 'cuerpo']:
        for cl, co in cols_lower.items():
            if kw in cl: resumen_w.value = co; break
    for kw in ['tono', 'tone', 'sentimiento', 'sentiment']:
        for cl, co in cols_lower.items():
            if kw in cl: tono_w.value = co; break
    for kw in ['tema', 'topic', 'categoria', 'categor√≠a', 'asunto', 'etiqueta']:
        for cl, co in cols_lower.items():
            if kw in cl: tema_w.value = co; break

# ============================================================================
# PASO 3: GENERAR ARCHIVO FINAL
# ============================================================================
def generate_final_file():
    """Procesa todos los DataFrames seg√∫n los mapeos y genera el archivo final."""
    with output_area:
        clear_output(wait=True)
        print("\n" + "="*70)
        print("‚öôÔ∏è PROCESANDO Y UNIENDO LOS ARCHIVOS SELECCIONADOS...")
        print("="*70 + "\n")

        if not uploaded_files:
            print("‚ùå No hay archivos para procesar. ¬°Todos fueron omitidos o no se cargaron!")
            print("\n" + "="*70)
            print("Puedes volver a ejecutar la celda para empezar de nuevo.")
            print("="*70)
            return

        all_dfs = []
        for filename, df in uploaded_files.items():
            mapping = all_mappings[filename]
            print(f"üìÑ Procesando {filename}... ", end='')

            titulo_col = mapping['titulo']
            resumen_col = mapping['resumen']
            if titulo_col != '(No usar)' and resumen_col != '(No usar)':
                resumen_expr = (pl.col(titulo_col).cast(pl.Utf8).fill_null("") + pl.lit(". ") + pl.col(resumen_col).cast(pl.Utf8).fill_null("")).alias('resumen')
            elif titulo_col != '(No usar)':
                resumen_expr = pl.col(titulo_col).cast(pl.Utf8).alias('resumen')
            elif resumen_col != '(No usar)':
                resumen_expr = pl.col(resumen_col).cast(pl.Utf8).alias('resumen')
            else:
                resumen_expr = pl.lit(None, dtype=pl.Utf8).alias('resumen')

            tono_col = mapping['tono']
            tono_expr = pl.col(tono_col).cast(pl.Utf8).alias('tono') if tono_col != '(No usar)' else pl.lit(None, dtype=pl.Utf8).alias('tono')

            tema_col = mapping['tema']
            tema_expr = pl.col(tema_col).cast(pl.Utf8).alias('tema') if tema_col != '(No usar)' else pl.lit(None, dtype=pl.Utf8).alias('tema')

            processed_df = df.select([resumen_expr, tono_expr, tema_expr, pl.lit(filename).alias('archivo_origen')])
            all_dfs.append(processed_df)
            print(f"‚úÖ {processed_df.height:,} filas a√±adidas.")

        print("\n‚ö° Concatenando todo...")
        if not all_dfs:
            print("‚ùå No hay datos para procesar.")
            return

        final_df = pl.concat(all_dfs, how='vertical')
        final_df = final_df.with_columns([pl.when(pl.col(c).str.strip_chars() == "").then(None).otherwise(pl.col(c)).name.keep() for c in ['resumen', 'tono', 'tema']])
        final_df = final_df.filter(pl.col('resumen').is_not_null())

        print(f"\n‚ú® ¬°PROCESO COMPLETADO!")
        print(f"üìä Total de filas en el archivo final: {final_df.height:,}\n")

        print("üëÄ VISTA PREVIA (10 primeras filas del archivo unificado):")
        print("-"*70)
        display(final_df.head(10).to_pandas())
        print("-"*70)

        print(f"\nüìà ESTAD√çSTICAS DEL RESULTADO:")
        print(f"   ‚Ä¢ Filas con 'resumen': {final_df.filter(pl.col('resumen').is_not_null()).height:,}")
        print(f"   ‚Ä¢ Filas con 'tono': {final_df.filter(pl.col('tono').is_not_null()).height:,}")
        print(f"   ‚Ä¢ Filas con 'tema': {final_df.filter(pl.col('tema').is_not_null()).height:,}")

        print("\nüíæ Generando archivo Excel para descargar...")
        output_filename = 'archivo_unificado.xlsx'
        final_df.write_excel(output_filename, worksheet='DatosUnificados')

        files.download(output_filename)
        print(f"\n‚úÖ ¬°Descarga iniciada! Revisa tus descargas para el archivo '{output_filename}'.")
        print("\n" + "="*70)
        print("üéâ ¬°TODO LISTO! PUEDES CERRAR ESTA VENTANA O VOLVER A EJECUTAR LA CELDA PARA UN NUEVO PROCESO.")
        print("="*70)

# ============================================================================
# INICIAR PROCESO
# ============================================================================
start_button = widgets.Button(
    description='üìÅ SUBIR ARCHIVOS Y COMENZAR',
    button_style='primary',
    icon='upload',
    layout=widgets.Layout(width='auto', height='50px'),
    style={'font_weight': 'bold'}
)
start_button.on_click(step1_upload)

display(
    widgets.VBox([
        widgets.HTML("<h1>üöÄ Concatenador XLSX Ultra-F√°cil üöÄ</h1>"
                     "<p>Esta herramienta te guiar√° para unir m√∫ltiples archivos Excel en uno solo, mapeando las columnas que necesites. Ahora con la opci√≥n de omitir archivos.</p>"),
        start_button,
        widgets.HTML("<hr>"),
        output_area
    ])
)

"""
=================================================================
#@title ENTRENADOR DE MODELOS PARA AN√ÅLISIS DE NOTICIAS
Notebook para Google Colab con GPU
Genera pipeline_sentimiento.pkl y pipeline_tema.pkl
=================================================================
"""

# ============================================
# PASO 1: INSTALACI√ìN DE DEPENDENCIAS
# ============================================
print("üì¶ Instalando dependencias...")
!pip install -q scikit-learn pandas openpyxl unidecode

# ============================================
# PASO 2: IMPORTS Y CONFIGURACI√ìN
# ============================================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import joblib
import re
from unidecode import unidecode
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Dependencias cargadas correctamente")

# ============================================
# PASO 3: FUNCIONES DE PREPROCESAMIENTO
# ============================================
STOPWORDS_ES = set("""
a ante bajo cabe con contra de desde durante en entre hacia hasta mediante
para por segun sin so sobre tras y o u e la el los las un una unos unas
lo al del se su sus le les mi mis tu tus nuestro nuestros vuestra vuestras
este esta estos estas ese esa esos esas aquel aquella aquellos aquellas
que cual cuales quien quienes cuyo cuya cuyos cuyas como cuando donde es
son fue fueron era eran sera seran seria serian he ha han habia habrian
hay hubo habra habria estoy esta estan estaba estaban estamos estar estare
estaria estuvieron estarian estuvo asi ya mas menos tan tanto cada
""".split())

def limpiar_texto(texto):
    """Limpia y normaliza el texto para el an√°lisis"""
    if pd.isna(texto) or texto == "":
        return ""

    # Convertir a string y lowercase
    texto = str(texto).lower()

    # Normalizar caracteres especiales
    texto = unidecode(texto)

    # Remover URLs
    texto = re.sub(r'http\S+|www\S+', '', texto)

    # Remover menciones y hashtags
    texto = re.sub(r'@\w+|#\w+', '', texto)

    # Remover caracteres especiales y n√∫meros
    texto = re.sub(r'[^a-z\s]', ' ', texto)

    # Remover espacios m√∫ltiples
    texto = re.sub(r'\s+', ' ', texto).strip()

    # Remover stopwords
    palabras = [p for p in texto.split() if p not in STOPWORDS_ES and len(p) > 2]

    return ' '.join(palabras)

# ============================================
# PASO 4: CARGA DE DATOS
# ============================================
print("\nüìÇ Por favor, sube tu archivo Excel con las columnas: 'resumen', 'tono', 'tema'")
uploaded = files.upload()

# Obtener el nombre del archivo
filename = list(uploaded.keys())[0]
print(f"\n‚úÖ Archivo '{filename}' cargado correctamente")

# Leer el archivo
df = pd.read_excel(filename)

print(f"\nüìä Dimensiones del dataset: {df.shape}")
print(f"Columnas disponibles: {df.columns.tolist()}")

# Verificar columnas necesarias
columnas_necesarias = ['resumen', 'tono', 'tema']
columnas_faltantes = [col for col in columnas_necesarias if col.lower() not in [c.lower() for c in df.columns]]

if columnas_faltantes:
    print(f"\n‚ö†Ô∏è ADVERTENCIA: Faltan columnas: {columnas_faltantes}")
    print("El archivo debe contener las columnas: 'resumen', 'tono', 'tema'")
    print("\nColumnas actuales detectadas:")
    for i, col in enumerate(df.columns):
        print(f"{i}: {col}")

    # Permitir mapeo manual
    print("\n¬øDeseas mapear las columnas manualmente? Ingresa los √≠ndices:")
    idx_resumen = int(input("√çndice de la columna 'resumen': "))
    idx_tono = int(input("√çndice de la columna 'tono': "))
    idx_tema = int(input("√çndice de la columna 'tema': "))

    df = df.rename(columns={
        df.columns[idx_resumen]: 'resumen',
        df.columns[idx_tono]: 'tono',
        df.columns[idx_tema]: 'tema'
    })

# Normalizar nombres de columnas
df.columns = [c.lower().strip() for c in df.columns]

# ============================================
# PASO 5: AN√ÅLISIS EXPLORATORIO
# ============================================
print("\n" + "="*60)
print("üìà AN√ÅLISIS EXPLORATORIO DE DATOS")
print("="*60)

# Eliminar filas con valores nulos en columnas cr√≠ticas
df_original = df.copy()
df = df.dropna(subset=['resumen'])

print(f"\nFilas con resumen v√°lido: {len(df)} de {len(df_original)}")

# Distribuci√≥n de Tono
if 'tono' in df.columns:
    print("\nüéØ Distribuci√≥n de TONO:")
    print(df['tono'].value_counts())
    print(f"\nClases √∫nicas: {df['tono'].nunique()}")

# Distribuci√≥n de Tema
if 'tema' in df.columns:
    print("\nüè∑Ô∏è Distribuci√≥n de TEMA (Top 20):")
    print(df['tema'].value_counts().head(20))
    print(f"\nClases √∫nicas: {df['tema'].nunique()}")

# ============================================
# PASO 6: PREPROCESAMIENTO
# ============================================
print("\n" + "="*60)
print("üîÑ PREPROCESAMIENTO DE DATOS")
print("="*60)

# Limpiar textos
print("\nLimpiando textos...")
df['resumen_limpio'] = df['resumen'].apply(limpiar_texto)

# Filtrar textos vac√≠os
df = df[df['resumen_limpio'].str.len() > 10]
print(f"‚úÖ Textos procesados: {len(df)}")

# ============================================
# PASO 7: SELECCI√ìN DE MODELO A ENTRENAR
# ============================================
print("\n" + "="*60)
print("‚öôÔ∏è CONFIGURACI√ìN DE ENTRENAMIENTO")
print("="*60)
print("\n¬øQu√© modelo deseas entrenar?")
print("1. Solo Sentimiento (pipeline_sentimiento.pkl)")
print("2. Solo Tema (pipeline_tema.pkl)")
print("3. Ambos modelos")

opcion = input("\nIngresa tu opci√≥n (1/2/3): ").strip()

entrenar_sentimiento = opcion in ['1', '3']
entrenar_tema = opcion in ['2', '3']

# ============================================
# PASO 8: ENTRENAMIENTO DE MODELO DE SENTIMIENTO
# ============================================
if entrenar_sentimiento and 'tono' in df.columns:
    print("\n" + "="*60)
    print("üéØ ENTRENAMIENTO: MODELO DE SENTIMIENTO")
    print("="*60)

    # Preparar datos
    df_sentimiento = df[['resumen_limpio', 'tono']].copy()
    df_sentimiento = df_sentimiento.dropna()

    # Normalizar etiquetas de tono
    mapeo_tono = {
        'positivo': 1, 'positive': 1, '1': 1, 1: 1,
        'neutro': 0, 'neutral': 0, '0': 0, 0: 0,
        'negativo': -1, 'negative': -1, '-1': -1, -1: -1
    }

    df_sentimiento['tono_normalizado'] = df_sentimiento['tono'].astype(str).str.lower().map(mapeo_tono)
    df_sentimiento = df_sentimiento.dropna(subset=['tono_normalizado'])

    X = df_sentimiento['resumen_limpio']
    y = df_sentimiento['tono_normalizado'].astype(int)

    print(f"\nüìä Dataset de sentimiento: {len(X)} muestras")
    print(f"Distribuci√≥n de clases:")
    print(y.value_counts())

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\n‚úÇÔ∏è Train: {len(X_train)} | Test: {len(X_test)}")

    # Pipelines 100% sklearn (compatibles con Streamlit)
    pipelines_sentimiento = {
        'Logistic Regression': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', C=1.0))
        ]),
        'LinearSVC': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', LinearSVC(max_iter=2000, random_state=42, class_weight='balanced', C=1.0))
        ]),
        'Random Forest': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=3000, ngram_range=(1, 2))),
            ('clf', RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', max_depth=20, n_jobs=-1))
        ])
    }

    # Entrenar y evaluar modelos
    print("\nüöÄ Entrenando modelos de sentimiento...")
    resultados_sentimiento = {}

    for nombre, pipeline in pipelines_sentimiento.items():
        print(f"\nüîπ Entrenando: {nombre}")
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        resultados_sentimiento[nombre] = {'pipeline': pipeline, 'accuracy': acc}
        print(f"   Accuracy: {acc:.4f}")

    # Seleccionar el mejor modelo
    mejor_modelo_sentimiento = max(resultados_sentimiento.items(), key=lambda x: x[1]['accuracy'])
    nombre_mejor = mejor_modelo_sentimiento[0]
    pipeline_sentimiento_final = mejor_modelo_sentimiento[1]['pipeline']
    acc_mejor = mejor_modelo_sentimiento[1]['accuracy']

    print(f"\nüèÜ Mejor modelo: {nombre_mejor} (Accuracy: {acc_mejor:.4f})")

    # Reporte detallado
    y_pred_final = pipeline_sentimiento_final.predict(X_test)
    print("\nüìã Reporte de clasificaci√≥n:")
    print(classification_report(y_test, y_pred_final,
                                target_names=['Negativo (-1)', 'Neutro (0)', 'Positivo (1)']))

    # Guardar modelo
    joblib.dump(pipeline_sentimiento_final, 'pipeline_sentimiento.pkl')
    print("\n‚úÖ Modelo guardado como 'pipeline_sentimiento.pkl'")

    # Ejemplo de predicci√≥n
    print("\nüîÆ Ejemplo de predicci√≥n:")
    textos_ejemplo = [
        "La empresa lanz√≥ un nuevo producto innovador que revolucionar√° el mercado",
        "Hubo problemas t√©cnicos graves que afectaron a miles de usuarios",
        "La compa√±√≠a present√≥ su informe trimestral de resultados"
    ]
    for texto in textos_ejemplo:
        texto_limpio = limpiar_texto(texto)
        pred = pipeline_sentimiento_final.predict([texto_limpio])[0]
        sentimiento = {1: "Positivo", 0: "Neutro", -1: "Negativo"}[pred]
        print(f"   '{texto[:60]}...' ‚Üí {sentimiento}")

# ============================================
# PASO 9: ENTRENAMIENTO DE MODELO DE TEMA
# ============================================
if entrenar_tema and 'tema' in df.columns:
    print("\n" + "="*60)
    print("üè∑Ô∏è ENTRENAMIENTO: MODELO DE TEMA")
    print("="*60)

    # Preparar datos
    df_tema = df[['resumen_limpio', 'tema']].copy()
    df_tema = df_tema.dropna()

    # Filtrar temas con suficientes muestras
    tema_counts = df_tema['tema'].value_counts()
    temas_validos = tema_counts[tema_counts >= 3].index
    df_tema = df_tema[df_tema['tema'].isin(temas_validos)]

    X = df_tema['resumen_limpio']
    y = df_tema['tema']

    print(f"\nüìä Dataset de temas: {len(X)} muestras")
    print(f"N√∫mero de temas √∫nicos: {y.nunique()}")
    print(f"\nTop 10 temas m√°s frecuentes:")
    print(y.value_counts().head(10))

    # Codificar etiquetas solo para comparar modelos
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    print(f"\n‚úÇÔ∏è Train: {len(X_train)} | Test: {len(X_test)}")

    # Pipeline para tema (sin SMOTE por muchas clases)
    pipelines_tema = {
        'Logistic Regression': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))
        ]),
        'LinearSVC': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LinearSVC(max_iter=2000, random_state=42, class_weight='balanced'))
        ]),
        'Multinomial NB': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', MultinomialNB(alpha=0.1))
        ])
    }

    # Entrenar y evaluar modelos
    print("\nüöÄ Entrenando modelos de tema...")
    resultados_tema = {}

    for nombre, pipeline in pipelines_tema.items():
        print(f"\nüîπ Entrenando: {nombre}")
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        resultados_tema[nombre] = {'pipeline': pipeline, 'accuracy': acc}
        print(f"   Accuracy: {acc:.4f}")

    # Seleccionar el mejor modelo
    mejor_modelo_tema = max(resultados_tema.items(), key=lambda x: x[1]['accuracy'])
    nombre_mejor_tema = mejor_modelo_tema[0]
    pipeline_tema_final = mejor_modelo_tema[1]['pipeline']
    acc_mejor_tema = mejor_modelo_tema[1]['accuracy']

    print(f"\nüèÜ Mejor modelo: {nombre_mejor_tema} (Accuracy: {acc_mejor_tema:.4f})")

    # SOLUCI√ìN REAL: Reentrenar con etiquetas de texto directamente
    print("\nüîÑ Reentrenando modelo con etiquetas de texto...")

    # Usar las etiquetas originales (strings) en lugar de n√∫meros
    X_train_texto, X_test_texto, y_train_texto, y_test_texto = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Recrear el mejor pipeline y entrenar con strings
    if nombre_mejor_tema == 'Logistic Regression':
        pipeline_final = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))
        ])
    elif nombre_mejor_tema == 'LinearSVC':
        pipeline_final = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LinearSVC(max_iter=2000, random_state=42, class_weight='balanced'))
        ])
    else:  # Multinomial NB
        pipeline_final = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', MultinomialNB(alpha=0.1))
        ])

    # Entrenar con strings directamente
    pipeline_final.fit(X_train_texto, y_train_texto)

    print("‚úÖ Modelo reentrenado con etiquetas de texto")

    # Reporte detallado (top 10 clases)
    y_pred_texto = pipeline_final.predict(X_test_texto)

    print("\nüìã Reporte de clasificaci√≥n (Top 10 temas):")
    top_10_temas = y.value_counts().head(10).index

    # Crear series sin √≠ndice para evitar problemas de alineaci√≥n
    y_test_series = pd.Series(y_test_texto.values if hasattr(y_test_texto, 'values') else list(y_test_texto))
    y_pred_series = pd.Series(y_pred_texto)

    mask = y_test_series.isin(top_10_temas)

    if mask.sum() > 0:
        print(classification_report(
            y_test_series[mask],
            y_pred_series[mask],
            labels=top_10_temas.tolist()
        ))

    # Guardar modelo (100% sklearn, sin wrappers)
    joblib.dump(pipeline_final, 'pipeline_tema.pkl')
    print("\n‚úÖ Modelo guardado como 'pipeline_tema.pkl'")
    print("‚ö†Ô∏è IMPORTANTE: Este modelo predice strings directamente (nombres de temas)")

    # Ejemplo de predicci√≥n
    print("\nüîÆ Ejemplo de predicci√≥n:")
    textos_ejemplo = [
        "La empresa lanz√≥ un nuevo producto innovador que revolucionar√° el mercado",
        "Resultados financieros del tercer trimestre superan expectativas",
        "Apertura de nueva sucursal en el centro de la ciudad"
    ]
    for texto in textos_ejemplo:
        texto_limpio = limpiar_texto(texto)
        pred = pipeline_final.predict([texto_limpio])[0]
        print(f"   '{texto[:60]}...' ‚Üí {pred}")

# ============================================
# PASO 10: DESCARGA DE MODELOS
# ============================================
print("\n" + "="*60)
print("üì• DESCARGA DE MODELOS")
print("="*60)

if entrenar_sentimiento:
    print("\n‚úÖ Descargando pipeline_sentimiento.pkl...")
    files.download('pipeline_sentimiento.pkl')

if entrenar_tema:
    print("\n‚úÖ Descargando pipeline_tema.pkl...")
    files.download('pipeline_tema.pkl')

print("\n" + "="*60)
print("üéâ PROCESO COMPLETADO")
print("="*60)
print("\nüìå Los modelos est√°n listos para usar en tu aplicaci√≥n Streamlit")
print("üìå Simplemente carga los archivos .pkl en el modo 'H√≠brido (PKL + API)'")
print("\nüí° Recuerda:")
print("   ‚Ä¢ pipeline_sentimiento.pkl predice: -1 (Negativo), 0 (Neutro), 1 (Positivo)")
print("   ‚Ä¢ pipeline_tema.pkl predice el nombre del tema como string")

"""# Explicaci√≥n

## Detalles

Este notebook consta de dos partes principales:

### 1. Concatenador y unificador de archivos XLSX

Este segmento de c√≥digo permite al usuario subir m√∫ltiples archivos XLSX, previsualizar sus contenidos y mapear columnas espec√≠ficas ('t√≠tulo', 'resumen', 'tono', 'tema') a un formato unificado. Incluye una funcionalidad para auto-detectar nombres de columnas comunes y la opci√≥n de omitir archivos. El resultado es un archivo Excel consolidado con las columnas estandarizadas, listo para ser usado como dataset de entrenamiento.

**Tecnolog√≠as clave:**
*   `polars`: Para la lectura y manipulaci√≥n eficiente de DataFrames, especialmente con archivos grandes.
*   `ipywidgets`: Para crear una interfaz de usuario interactiva y amigable directamente en Colab.
*   `google.colab.files`: Para la gesti√≥n de subida y descarga de archivos en el entorno de Colab.

### 2. Entrenador de modelos para an√°lisis de noticias

Esta secci√≥n se encarga de entrenar modelos de Machine Learning para clasificar el **tono** y el **tema** de textos (res√∫menes de noticias). Utiliza el archivo consolidado generado en la primera parte como datos de entrada.

**Flujo de trabajo:**
1.  **Carga de datos:** Se sube el archivo Excel unificado.
2.  **Preprocesamiento:** Los textos se limpian (min√∫sculas, eliminaci√≥n de stopwords, caracteres especiales, URLs, menciones y hashtags) utilizando la biblioteca `unidecode` para normalizar acentos y `re` para expresiones regulares.
3.  **An√°lisis exploratorio:** Se muestra la distribuci√≥n de las categor√≠as de 'tono' y 'tema'.
4.  **Selecci√≥n de modelos:** El usuario elige si entrenar el modelo de sentimiento, el de tema o ambos.
5.  **Entrenamiento:**
    *   **Modelo de Sentimiento:** Clasifica textos en 'Positivo', 'Neutro' o 'Negativo'. Se entrena con varios modelos (Regresi√≥n Log√≠stica, LinearSVC, Random Forest) y se selecciona el de mejor rendimiento. Las etiquetas num√©ricas (-1, 0, 1) se asignan internamente. El modelo final se guarda como `pipeline_sentimiento.pkl`.
    *   **Modelo de Tema:** Clasifica textos en diferentes temas. Tambi√©n se entrena con varios modelos (Regresi√≥n Log√≠stica, LinearSVC, Multinomial Naive Bayes) y se selecciona el mejor. **Es crucial que este modelo se reentrena al final con las etiquetas de tema originales (strings) para que las predicciones sean directamente los nombres de los temas.** El modelo final se guarda como `pipeline_tema.pkl`.
6.  **Evaluaci√≥n y Ejemplos:** Se muestra un reporte de clasificaci√≥n y ejemplos de predicci√≥n.
7.  **Descarga:** Los modelos entrenados (`.pkl`) se descargan para su uso posterior en aplicaciones como Streamlit.

**Tecnolog√≠as clave:**
*   `pandas`: Para la manipulaci√≥n de datos en DataFrames.
*   `scikit-learn`: El pilar para el Machine Learning, incluyendo `TfidfVectorizer` para la vectorizaci√≥n de texto, `train_test_split` para dividir datos, y diversos clasificadores como `LogisticRegression`, `LinearSVC`, `RandomForestClassifier`, `MultinomialNB`, y `Pipeline` para encapsular todo el proceso.
*   `joblib`: Para guardar y cargar los modelos entrenados (`.pkl`).
*   `unidecode`: Para la normalizaci√≥n de texto.

### Posibles Mejoras e Implementaciones Futuras

*   **Balanceo de Clases:** Aunque se usa `class_weight='balanced'` en algunos modelos, para clases muy desequilibradas (como el tono negativo o algunos temas poco frecuentes), se podr√≠an explorar t√©cnicas m√°s avanzadas como `SMOTE` (Synthetic Minority Over-sampling Technique) para generar muestras sint√©ticas de la clase minoritaria, especialmente en el entrenamiento del modelo de sentimiento.
*   **Validaci√≥n Cruzada:** Implementar una validaci√≥n cruzada m√°s robusta para una evaluaci√≥n m√°s fiable del modelo, especialmente al seleccionar hiperpar√°metros (GridSearchCV ya la incluye, pero se podr√≠a usar `cross_val_score` para una evaluaci√≥n general).
*   **M√°s Algoritmos:** Experimentar con otros algoritmos de clasificaci√≥n como Gradient Boosting (XGBoost, LightGBM) o incluso modelos basados en redes neuronales (Deep Learning) si la cantidad de datos lo justifica y se cuenta con recursos computacionales (GPU).
*   **Embeddings de Palabras:** Utilizar embeddings de palabras preentrenados (como Word2Vec, GloVe, FastText) o contextuales (como BERT, RoBERTa) en lugar de TF-IDF, lo que a menudo mejora significativamente el rendimiento en tareas de clasificaci√≥n de texto. Esto requerir√≠a una capa adicional en el pipeline o una arquitectura de red neuronal.
*   **Aumento de Datos (Data Augmentation):** Para el texto, se podr√≠an generar m√°s ejemplos de entrenamiento mediante t√©cnicas como sin√≥nimos, par√°frasis o traducciones inversas, lo que es √∫til especialmente con datasets peque√±os.
*   **Preprocesamiento Adicional:** A√±adir lematizaci√≥n o stemming a la funci√≥n `limpiar_texto` para reducir las palabras a su ra√≠z y manejar mejor las variaciones morfol√≥gicas.
*   **Interfaz de Usuario Mejorada:** Para el concatenador XLSX, se podr√≠a a√±adir una barra de progreso o m√°s retroalimentaci√≥n visual durante la carga y procesamiento de archivos grandes. Para el entrenador, una interfaz m√°s guiada para la selecci√≥n de hiperpar√°metros podr√≠a ser √∫til.
*   **Serializaci√≥n de LabelEncoder:** Cuando se entrena el modelo de tema con `LabelEncoder`, es fundamental guardar y cargar el `LabelEncoder` junto con el pipeline. Aunque el c√≥digo actual reentrena el modelo de tema con las etiquetas de texto directamente, si se volviera a usar `LabelEncoder`, ser√≠a un punto cr√≠tico.
*   **Manejo de Errores Robustos:** Mejorar el manejo de errores en cada paso, proporcionando mensajes m√°s claros al usuario en caso de fallos inesperados.
*   **Almacenamiento en la Nube:** Integrar el almacenamiento directo de los modelos `.pkl` en Google Drive o Google Cloud Storage para facilitar su acceso y versionado.
"""

"""
=================================================================
#@title ENTRENADOR DE MODELOS PARA AN√ÅLISIS DE NOTICIAS
Notebook para Google Colab con GPU
Genera pipeline_sentimiento.pkl y pipeline_tema.pkl
=================================================================
"""

# ============================================
# PASO 1: INSTALACI√ìN DE DEPENDENCIAS
# ============================================
print("üì¶ Instalando dependencias...")
!pip install -q scikit-learn pandas openpyxl unidecode

# ============================================
# PASO 2: IMPORTS Y CONFIGURACI√ìN
# ============================================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import joblib
import re
from unidecode import unidecode
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Dependencias cargadas correctamente")

# ============================================
# PASO 3: FUNCIONES DE PREPROCESAMIENTO
# ============================================
STOPWORDS_ES = set("""
a ante bajo cabe con contra de desde durante en entre hacia hasta mediante
para por segun sin so sobre tras y o u e la el los las un una unos unas
lo al del se su sus le les mi mis tu tus nuestro nuestros vuestra vuestras
este esta estos estas ese esa esos esas aquel aquella aquellos aquellas
que cual cuales quien quienes cuyo cuya cuyos cuyas como cuando donde es
son fue fueron era eran sera seran seria serian he ha han habia habrian
hay hubo habra habria estoy esta estan estaba estaban estamos estar estare
estaria estuvieron estarian estuvo asi ya mas menos tan tanto cada
""".split())

def limpiar_texto(texto):
    """Limpia y normaliza el texto para el an√°lisis"""
    if pd.isna(texto) or texto == "":
        return ""

    # Convertir a string y lowercase
    texto = str(texto).lower()

    # Normalizar caracteres especiales
    texto = unidecode(texto)

    # Remover URLs
    texto = re.sub(r'http\S+|www\S+', '', texto)

    # Remover menciones y hashtags
    texto = re.sub(r'@\w+|#\w+', '', texto)

    # Remover caracteres especiales y n√∫meros
    texto = re.sub(r'[^a-z\s]', ' ', texto)

    # Remover espacios m√∫ltiples
    texto = re.sub(r'\s+', ' ', texto).strip()

    # Remover stopwords
    palabras = [p for p in texto.split() if p not in STOPWORDS_ES and len(p) > 2]

    return ' '.join(palabras)

# ============================================
# PASO 4: CARGA DE DATOS
# ============================================
print("\nüìÇ Por favor, sube tu archivo Excel con las columnas: 'resumen', 'tono', 'tema'")
uploaded = files.upload()

# Obtener el nombre del archivo
filename = list(uploaded.keys())[0]
print(f"\n‚úÖ Archivo '{filename}' cargado correctamente")

# Leer el archivo
df = pd.read_excel(filename)

print(f"\nüìä Dimensiones del dataset: {df.shape}")
print(f"Columnas disponibles: {df.columns.tolist()}")

# Verificar columnas necesarias
columnas_necesarias = ['resumen', 'tono', 'tema']
columnas_faltantes = [col for col in columnas_necesarias if col.lower() not in [c.lower() for c in df.columns]]

if columnas_faltantes:
    print(f"\n‚ö†Ô∏è ADVERTENCIA: Faltan columnas: {columnas_faltantes}")
    print("El archivo debe contener las columnas: 'resumen', 'tono', 'tema'")
    print("\nColumnas actuales detectadas:")
    for i, col in enumerate(df.columns):
        print(f"{i}: {col}")

    # Permitir mapeo manual
    print("\n¬øDeseas mapear las columnas manualmente? Ingresa los √≠ndices:")
    idx_resumen = int(input("√çndice de la columna 'resumen': "))
    idx_tono = int(input("√çndice de la columna 'tono': "))
    idx_tema = int(input("√çndice de la columna 'tema': "))

    df = df.rename(columns={
        df.columns[idx_resumen]: 'resumen',
        df.columns[idx_tono]: 'tono',
        df.columns[idx_tema]: 'tema'
    })

# Normalizar nombres de columnas
df.columns = [c.lower().strip() for c in df.columns]

# ============================================
# PASO 5: AN√ÅLISIS EXPLORATORIO
# ============================================
print("\n" + "="*60)
print("üìà AN√ÅLISIS EXPLORATORIO DE DATOS")
print("="*60)

# Eliminar filas con valores nulos en columnas cr√≠ticas
df_original = df.copy()
df = df.dropna(subset=['resumen'])

print(f"\nFilas con resumen v√°lido: {len(df)} de {len(df_original)}")

# Distribuci√≥n de Tono
if 'tono' in df.columns:
    print("\nüéØ Distribuci√≥n de TONO:")
    print(df['tono'].value_counts())
    print(f"\nClases √∫nicas: {df['tono'].nunique()}")

# Distribuci√≥n de Tema
if 'tema' in df.columns:
    print("\nüè∑Ô∏è Distribuci√≥n de TEMA (Top 20):")
    print(df['tema'].value_counts().head(20))
    print(f"\nClases √∫nicas: {df['tema'].nunique()}")

# ============================================
# PASO 6: PREPROCESAMIENTO
# ============================================
print("\n" + "="*60)
print("üîÑ PREPROCESAMIENTO DE DATOS")
print("="*60)

# Limpiar textos
print("\nLimpiando textos...")
df['resumen_limpio'] = df['resumen'].apply(limpiar_texto)

# Filtrar textos vac√≠os
df = df[df['resumen_limpio'].str.len() > 10]
print(f"‚úÖ Textos procesados: {len(df)}")

# ============================================
# PASO 7: SELECCI√ìN DE MODELO A ENTRENAR
# ============================================
print("\n" + "="*60)
print("‚öôÔ∏è CONFIGURACI√ìN DE ENTRENAMIENTO")
print("="*60)
print("\n¬øQu√© modelo deseas entrenar?")
print("1. Solo Sentimiento (pipeline_sentimiento.pkl)")
print("2. Solo Tema (pipeline_tema.pkl)")
print("3. Ambos modelos")

opcion = input("\nIngresa tu opci√≥n (1/2/3): ").strip()

entrenar_sentimiento = opcion in ['1', '3']
entrenar_tema = opcion in ['2', '3']

# ============================================
# PASO 8: ENTRENAMIENTO DE MODELO DE SENTIMIENTO
# ============================================
if entrenar_sentimiento and 'tono' in df.columns:
    print("\n" + "="*60)
    print("üéØ ENTRENAMIENTO: MODELO DE SENTIMIENTO")
    print("="*60)

    # Preparar datos
    df_sentimiento = df[['resumen_limpio', 'tono']].copy()
    df_sentimiento = df_sentimiento.dropna()

    # Normalizar etiquetas de tono
    mapeo_tono = {
        'positivo': 1, 'positive': 1, '1': 1, 1: 1,
        'neutro': 0, 'neutral': 0, '0': 0, 0: 0,
        'negativo': -1, 'negative': -1, '-1': -1, -1: -1
    }

    df_sentimiento['tono_normalizado'] = df_sentimiento['tono'].astype(str).str.lower().map(mapeo_tono)
    df_sentimiento = df_sentimiento.dropna(subset=['tono_normalizado'])

    X = df_sentimiento['resumen_limpio']
    y = df_sentimiento['tono_normalizado'].astype(int)

    print(f"\nüìä Dataset de sentimiento: {len(X)} muestras")
    print(f"Distribuci√≥n de clases:")
    print(y.value_counts())

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\n‚úÇÔ∏è Train: {len(X_train)} | Test: {len(X_test)}")

    # Pipelines 100% sklearn (compatibles con Streamlit)
    pipelines_sentimiento = {
        'Logistic Regression': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', C=1.0))
        ]),
        'LinearSVC': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', LinearSVC(max_iter=2000, random_state=42, class_weight='balanced', C=1.0))
        ]),
        'Random Forest': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=3000, ngram_range=(1, 2))),
            ('clf', RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', max_depth=20, n_jobs=-1))
        ])
    }

    # Entrenar y evaluar modelos
    print("\nüöÄ Entrenando modelos de sentimiento...")
    resultados_sentimiento = {}

    for nombre, pipeline in pipelines_sentimiento.items():
        print(f"\nüîπ Entrenando: {nombre}")
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        resultados_sentimiento[nombre] = {'pipeline': pipeline, 'accuracy': acc}
        print(f"   Accuracy: {acc:.4f}")

    # Seleccionar el mejor modelo
    mejor_modelo_sentimiento = max(resultados_sentimiento.items(), key=lambda x: x[1]['accuracy'])
    nombre_mejor = mejor_modelo_sentimiento[0]
    pipeline_sentimiento_final = mejor_modelo_sentimiento[1]['pipeline']
    acc_mejor = mejor_modelo_sentimiento[1]['accuracy']

    print(f"\nüèÜ Mejor modelo: {nombre_mejor} (Accuracy: {acc_mejor:.4f})")

    # Reporte detallado
    y_pred_final = pipeline_sentimiento_final.predict(X_test)
    print("\nüìã Reporte de clasificaci√≥n:")
    print(classification_report(y_test, y_pred_final,
                                target_names=['Negativo (-1)', 'Neutro (0)', 'Positivo (1)']))

    # Guardar modelo
    joblib.dump(pipeline_sentimiento_final, 'pipeline_sentimiento.pkl')
    print("\n‚úÖ Modelo guardado como 'pipeline_sentimiento.pkl'")

    # Ejemplo de predicci√≥n
    print("\nüîÆ Ejemplo de predicci√≥n:")
    textos_ejemplo = [
        "La empresa lanz√≥ un nuevo producto innovador que revolucionar√° el mercado",
        "Hubo problemas t√©cnicos graves que afectaron a miles de usuarios",
        "La compa√±√≠a present√≥ su informe trimestral de resultados"
    ]
    for texto in textos_ejemplo:
        texto_limpio = limpiar_texto(texto)
        pred = pipeline_sentimiento_final.predict([texto_limpio])[0]
        sentimiento = {1: "Positivo", 0: "Neutro", -1: "Negativo"}[pred]
        print(f"   '{texto[:60]}...' ‚Üí {sentimiento}")

# ============================================
# PASO 9: ENTRENAMIENTO DE MODELO DE TEMA
# ============================================
if entrenar_tema and 'tema' in df.columns:
    print("\n" + "="*60)
    print("üè∑Ô∏è ENTRENAMIENTO: MODELO DE TEMA")
    print("="*60)

    # Preparar datos
    df_tema = df[['resumen_limpio', 'tema']].copy()
    df_tema = df_tema.dropna()

    # Filtrar temas con suficientes muestras
    tema_counts = df_tema['tema'].value_counts()
    temas_validos = tema_counts[tema_counts >= 3].index
    df_tema = df_tema[df_tema['tema'].isin(temas_validos)]

    X = df_tema['resumen_limpio']
    y = df_tema['tema']

    print(f"\nüìä Dataset de temas: {len(X)} muestras")
    print(f"N√∫mero de temas √∫nicos: {y.nunique()}")
    print(f"\nTop 10 temas m√°s frecuentes:")
    print(y.value_counts().head(10))

    # Codificar etiquetas solo para comparar modelos
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    print(f"\n‚úÇÔ∏è Train: {len(X_train)} | Test: {len(X_test)}")

    # Pipeline para tema (sin SMOTE por muchas clases)
    pipelines_tema = {
        'Logistic Regression': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))
        ]),
        'LinearSVC': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LinearSVC(max_iter=2000, random_state=42, class_weight='balanced'))
        ]),
        'Multinomial NB': Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', MultinomialNB(alpha=0.1))
        ])
    }

    # Entrenar y evaluar modelos
    print("\nüöÄ Entrenando modelos de tema...")
    resultados_tema = {}

    for nombre, pipeline in pipelines_tema.items():
        print(f"\nüîπ Entrenando: {nombre}")
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        resultados_tema[nombre] = {'pipeline': pipeline, 'accuracy': acc}
        print(f"   Accuracy: {acc:.4f}")

    # Seleccionar el mejor modelo
    mejor_modelo_tema = max(resultados_tema.items(), key=lambda x: x[1]['accuracy'])
    nombre_mejor_tema = mejor_modelo_tema[0]
    pipeline_tema_final = mejor_modelo_tema[1]['pipeline']
    acc_mejor_tema = mejor_modelo_tema[1]['accuracy']

    print(f"\nüèÜ Mejor modelo: {nombre_mejor_tema} (Accuracy: {acc_mejor_tema:.4f})")

    # SOLUCI√ìN REAL: Reentrenar con etiquetas de texto directamente
    print("\nüîÑ Reentrenando modelo con etiquetas de texto...")

    # Usar las etiquetas originales (strings) en lugar de n√∫meros
    X_train_texto, X_test_texto, y_train_texto, y_test_texto = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Recrear el mejor pipeline y entrenar con strings
    if nombre_mejor_tema == 'Logistic Regression':
        pipeline_final = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))
        ])
    elif nombre_mejor_tema == 'LinearSVC':
        pipeline_final = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1, 3))),
            ('clf', LinearSVC(max_iter=2000, random_state=42, class_weight='balanced'))
        ])
    else:  # Multinomial NB
        pipeline_final = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ('clf', MultinomialNB(alpha=0.1))
        ])

    # Entrenar con strings directamente
    pipeline_final.fit(X_train_texto, y_train_texto)

    print("‚úÖ Modelo reentrenado con etiquetas de texto")

    # Reporte detallado (top 10 clases)
    y_pred_texto = pipeline_final.predict(X_test_texto)

    print("\nüìã Reporte de clasificaci√≥n (Top 10 temas):")
    top_10_temas = y.value_counts().head(10).index

    # Crear series sin √≠ndice para evitar problemas de alineaci√≥n
    y_test_series = pd.Series(y_test_texto.values if hasattr(y_test_texto, 'values') else list(y_test_texto))
    y_pred_series = pd.Series(y_pred_texto)

    mask = y_test_series.isin(top_10_temas)

    if mask.sum() > 0:
        print(classification_report(
            y_test_series[mask],
            y_pred_series[mask],
            labels=top_10_temas.tolist()
        ))

    # Guardar modelo (100% sklearn, sin wrappers)
    joblib.dump(pipeline_final, 'pipeline_tema.pkl')
    print("\n‚úÖ Modelo guardado como 'pipeline_tema.pkl'")
    print("‚ö†Ô∏è IMPORTANTE: Este modelo predice strings directamente (nombres de temas)")

    # Ejemplo de predicci√≥n
    print("\nüîÆ Ejemplo de predicci√≥n:")
    textos_ejemplo = [
        "La empresa lanz√≥ un nuevo producto innovador que revolucionar√° el mercado",
        "Resultados financieros del tercer trimestre superan expectativas",
        "Apertura de nueva sucursal en el centro de la ciudad"
    ]
    for texto in textos_ejemplo:
        texto_limpio = limpiar_texto(texto)
        pred = pipeline_final.predict([texto_limpio])[0]
        print(f"   '{texto[:60]}...' ‚Üí {pred}")

# ============================================
# PASO 10: DESCARGA DE MODELOS
# ============================================
print("\n" + "="*60)
print("üì• DESCARGA DE MODELOS")
print("="*60)

if entrenar_sentimiento:
    print("\n‚úÖ Descargando pipeline_sentimiento.pkl...")
    files.download('pipeline_sentimiento.pkl')

if entrenar_tema:
    print("\n‚úÖ Descargando pipeline_tema.pkl...")
    files.download('pipeline_tema.pkl')

print("\n" + "="*60)
print("üéâ PROCESO COMPLETADO")
print("="*60)
print("\nüìå Los modelos est√°n listos para usar en tu aplicaci√≥n Streamlit")
print("üìå Simplemente carga los archivos .pkl en el modo 'H√≠brido (PKL + API)'")
print("\nüí° Recuerda:")
print("   ‚Ä¢ pipeline_sentimiento.pkl predice: -1 (Negativo), 0 (Neutro), 1 (Positivo)")
print("   ‚Ä¢ pipeline_tema.pkl predice el nombre del tema como string")